{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970716e7-04fd-49cf-bc5c-69dc787e9b5f",
   "metadata": {},
   "source": [
    "# Welcome Sam Altman to Bangalore\n",
    "You ask Sam Altman for 2.2 billion USD in funding, and he makes fun of you in public\n",
    "\n",
    "He asks you where you live, and finally, after talking to you, gives you a challenging task\n",
    "\n",
    "Collect raw Instagram data of all OpenAI followers\n",
    "\n",
    "Answer these questions:\n",
    "\n",
    "- Who has the maximum posts\n",
    "- who has the maximum followers\n",
    "- Who follows the maximum people\n",
    "- How many categories (Digital creators, Non-profit foundation, etc do we have? How many people do we have?\n",
    "\n",
    "You have 24 hours!\n",
    "\n",
    "You then ask for data - he laughs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d7b6da-b344-4c0c-95d8-07281895044c",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "You hire Vijyalaxmi Iyer and Sam Chandra to help with the project. \n",
    "\n",
    "Vijyalaxmi lives all the way in Hebbal, and Sam stays in HSR Layout, so meeting at either place would be a long trip for someone. \n",
    "To keep things fair, you decide to set up a meeting at Rameshwaram CafÃ© in Indiranagar, a perfect midway point.\n",
    "\n",
    "Three of you sit down and lay out the entire project plan. You break the work into two major parts:\n",
    "\n",
    "- Laxmi and Sam will be in charge of collecting the Instagram follower data.\n",
    "- You will build the Python Script that will process everything.\n",
    "  \n",
    "By the end of the meeting, everyone is aligned: Laxmi and Sam will focus on accurate data collection, and youâ€™ll focus on making sure the code can handle real-world data seamlessly. The team is ready, the plan is set, and the project officially kicks off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d7f00-ba60-4ae0-be17-08aefae7239f",
   "metadata": {},
   "source": [
    "# Parsing Data in Pure Python\n",
    "\n",
    "While Laxmi and Sam are collecting the real data, we continue building the core of our project using a simple demo dataset. This lets us solidify the parsing logic so that the moment the actual field data arrives, the program will already be ready to handle it.\n",
    "\n",
    "We begin by loading the demo text file. The file contains multiple profiles, each separated by an empty line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8921a7c4-6206-4ee1-994b-0b9a06aedeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open File or row data\n",
    "with open(\"finaldata.txt\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9038419-e787-4684-9781-d373e32d63f0",
   "metadata": {},
   "source": [
    "The next step is to break the data into individual profile blocks. Each profile in the file is separated by two newline characters, so splitting on that pattern gives us separate units to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c3f3aa-2368-4bec-991a-9f6d3391176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Row data into chunks\n",
    "chunks = data.split(\"\\n\\n\")\n",
    "chunks = [c for c in chunks if len(chunks)>3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f53043-79ea-46ef-a009-741767ff2333",
   "metadata": {},
   "source": [
    "This helps us see the structure of each block before writing the parsing logic.\n",
    "\n",
    "The goal is to convert each chunk from raw text into a clean Python dictionary. For that, we define a function that trims extra whitespace, separates the lines, removes labels, and returns the meaningful parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573912b2-6675-4531-a3e5-d7c46b2fc32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Aman', 'Username': 'cubbonpark_runner', 'No. of Posts': 95, 'Followers': 1500, 'Following': 40, 'Type of Page': 'Lifestyle', 'Bio': 'ðŸƒ Running after coding\\nðŸ“ Cubbon Park\\nstrava.com/amanrunning'}\n"
     ]
    }
   ],
   "source": [
    "# Make function to parse the data (Clean Data)\n",
    "\n",
    "def parse_data(chunk):\n",
    "    username = \"\"\n",
    "    no_of_posts = 0\n",
    "    followers = 0\n",
    "    following = 0\n",
    "    name = \"\"\n",
    "    type_of_page = \"Unknown\"\n",
    "    bio = \"\"\n",
    "    \n",
    "    try:\n",
    "        chunk = chunk.strip()\n",
    "        sep_chunk = chunk.split(\"\\n\")\n",
    "        \n",
    "        if(len(sep_chunk)>0):\n",
    "            username = sep_chunk[0]\n",
    "        if(len(sep_chunk)>1):\n",
    "            no_of_posts = int(sep_chunk[1].split(\" posts\")[0].split(\" post\")[0].replace(\",\", \"\"))\n",
    "        if(len(sep_chunk)>2):\n",
    "            followers = float(sep_chunk[2].split(\" followers\")[0].replace(\",\", \"\").replace(\"K\", \"\").replace(\"M\", \"\"))\n",
    "            if(\"K\" in sep_chunk[2]):\n",
    "                followers = int(followers * 1000)\n",
    "            elif(\"M\" in sep_chunk[2]):\n",
    "                followers = int(followers * 1000000)\n",
    "            else:\n",
    "                followers = int(followers)\n",
    "        if(len(sep_chunk)>3):\n",
    "            following = float(sep_chunk[3].split(\" following\")[0].replace(\",\",\"\").replace(\"K\", \"\").replace(\"M\", \"\"))\n",
    "            if \"K\" in sep_chunk[3]:\n",
    "                following = int(following * 1000)\n",
    "            elif \"M\" in sep_chunk[3]:\n",
    "                following = int(following * 1000000)\n",
    "            else:\n",
    "                following = int(following)\n",
    "        if(len(sep_chunk)>4):\n",
    "            name = sep_chunk[4]\n",
    "        if(len(sep_chunk)>5):\n",
    "            type_of_page = sep_chunk[5]\n",
    "        if(len(sep_chunk)>6):\n",
    "            bio = \"\\n\".join(sep_chunk[6:])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return {\"Name\":name, \"Username\":username, \"No. of Posts\":no_of_posts, \"Followers\":followers, \"Following\":following, \"Type of Page\":type_of_page, \"Bio\":bio}\n",
    "\n",
    "print(parse_data(chunks[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1c49a9-267b-4b06-9466-23e54897a9b3",
   "metadata": {},
   "source": [
    "Running the function on a sample chunk gives a structured dictionary containing the name, posts, followers and bio fields extracted from the original text.\n",
    "\n",
    "So far, we have a stable method to read demo data from a file, split it into separate profile blocks, and parse each block into a structured format using only basic Python. This sets up the foundation for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c6c937-3f4c-4c53-bd51-2f8d19e93ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect alll chunks in dictionary\n",
    "all_chunks = []\n",
    "for chunk in chunks:\n",
    "    parse_chunks = parse_data(chunk)\n",
    "    all_chunks.append(parse_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec055c9-f23a-440b-89a5-bc524cf745c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "clean_file = json.dumps(all_chunks, indent=4)\n",
    "with open(\"data2.json\", \"w\") as f:\n",
    "    f.write(clean_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ccbbe4-78a0-4fda-aca1-fe8039c23a92",
   "metadata": {},
   "source": [
    "# Finding People with Max Posts and Followers\n",
    "\n",
    "- Who has the maximum posts\n",
    "- who has the maximum followers\n",
    "- Who follows the maximum people\n",
    "- How many categories (Digital creators, Non-profit foundation, etc do we have? How many people do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "572f0daf-ff5a-44c7-aef5-7fe12b3a5ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startuphub_blr\n"
     ]
    }
   ],
   "source": [
    "# Who has the maximum posts\n",
    "max = 0\n",
    "for chunk in all_chunks:\n",
    "    if (max<chunk['No. of Posts']):\n",
    "        max = chunk['No. of Posts']\n",
    "        max_no_of_post = chunk\n",
    "print(max_no_of_post['Username'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9fa375e-ca92-437f-846a-6e5ad4aceaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_anujsinghal : 681000\n"
     ]
    }
   ],
   "source": [
    "# who has the maximum followers\n",
    "max = 0\n",
    "for chunk in all_chunks:\n",
    "    if max < chunk['Followers']:\n",
    "        max = chunk['Followers']\n",
    "        max_followers = chunk\n",
    "print(f\"{max_followers['Username']} : {max_followers['Followers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67fb4cb6-be39-415d-8bc5-6eb1735fbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bangalore_tech_bro : 890\n"
     ]
    }
   ],
   "source": [
    "# Who follows the maximum people\n",
    "max = 0\n",
    "for chunk in all_chunks:\n",
    "    if max < chunk['Following']:\n",
    "        max = chunk['Following']\n",
    "        max_following = chunk\n",
    "print(f\"{max_following['Username']} : {max_following['Following']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "157c3901-a813-4f65-9799-2c4cffcf44af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categories : 34\n"
     ]
    }
   ],
   "source": [
    "# How many Category we have\n",
    "max = set()\n",
    "for chunk in all_chunks:\n",
    "    max.add(chunk['Type of Page'])\n",
    "    \n",
    "print(f\"Total Categories : {len(max)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
